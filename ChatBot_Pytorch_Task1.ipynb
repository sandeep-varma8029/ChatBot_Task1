{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qVWdzSbkGP3",
        "outputId": "1ecd7dcd-ae4c-469d-ce4c-dd49d97a9de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np  # For numerical operations\n",
        "import random  # For generating random numbers\n",
        "import json  # For working with JSON files\n",
        "import nltk  # For natural language processing tasks\n",
        "import torch  # For creating and training deep learning models\n",
        "import torch.nn as nn  # For defining neural network architectures\n",
        "from torch.utils.data import Dataset, DataLoader  # For loading and processing datasets efficiently\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "from typing import List  # Import the List type hint for type annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the intents JSON file in read mode\n",
        "with open('intents.json', 'r') as f:\n",
        "    # Load the contents of the file into a dictionary\n",
        "    intents = json.load(f)"
      ],
      "metadata": {
        "id": "ey7q6Fp9e736"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# function to tokenize a sentence\n",
        "def tokenize(sentence):\n",
        "    # Use the NLTK word_tokenize function to tokenize the sentence\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    return tokens\n",
        "\n",
        "# function to stem a word\n",
        "def stem(word):\n",
        "    # Use the PorterStemmer algorithm to stem the word\n",
        "    stemmed_word = stemmer.stem(word.lower())\n",
        "    return stemmed_word"
      ],
      "metadata": {
        "id": "0H444LcpkVjn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bag_of_words(tokenized_sentence: List[str], words: List[str]) -> np.ndarray:\n",
        "    # Stem each word in the tokenized sentence\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    \n",
        "    # Initialize the bag with zeros for each word in the list of known words\n",
        "    bag = np.zeros(len(words), dtype=np.float32)\n",
        "    \n",
        "    # Loop through each word in the list of known words and check if it's in the stemmed sentence\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words: \n",
        "            bag[idx] = 1\n",
        "    \n",
        "    return bag\n"
      ],
      "metadata": {
        "id": "stJr9F5skcbk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes three empty lists to store the words, tags, and (words, tag) pairs that will be used to train a natural language processing model. It then loops through each intent in the intents dictionary and extracts the tag associated with that intent. The tag is added to the list of tags. The code then loops through each pattern associated with the intent and tokenizes the pattern into individual words using the tokenize function. The words are added to the list of all words, and the (words, tag) pair is added to the list of pairs. This process is repeated for each intent and pattern in the intents dictionary. The resulting all_words, tags, and xy lists can be used to train a model to understand natural language input and generate appropriate responses.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IGCqYWFTfsb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty lists to store the words, tags, and (words, tag) pairs\n",
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "\n",
        "# Loop through each intent in the `intents` dictionary\n",
        "for intent in intents['intents']:\n",
        "    # Extract the tag from the intent\n",
        "    tag = intent['tag']\n",
        "    # Add the tag to the list of tags\n",
        "    tags.append(tag)\n",
        "    # Loop through each pattern in the intent\n",
        "    for pattern in intent['patterns']:\n",
        "        # Tokenize the pattern into individual words\n",
        "        words = tokenize(pattern)\n",
        "        # Add the words to the list of all words\n",
        "        all_words.extend(words)\n",
        "        # Add the (words, tag) pair to the list of pairs\n",
        "        xy.append((words, tag))\n"
      ],
      "metadata": {
        "id": "l-gvLs5EkkXB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of words to ignore\n",
        "ignore_words = ['?', '.', '!']\n",
        "\n",
        "# Stem and lower each word in the list of all words, and exclude the words in the ignore list\n",
        "all_words = [stem(w.lower()) for w in all_words if w not in ignore_words]\n",
        "\n",
        "# Remove duplicates and sort the list of all words\n",
        "all_words = sorted(set(all_words))\n",
        "\n",
        "# Sort the list of tags\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "# Print some information about the dataset\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd19PS3ckrxx",
        "outputId": "48e6d103-b010-4c77-b66f-4c1fc31e16c4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56 patterns\n",
            "11 tags: ['contact', 'delivery', 'funny', 'goodbye', 'greeting', 'help', 'items', 'name', 'payments', 'thanks', 'weather']\n",
            "104 unique stemmed words: [\"'m\", \"'s\", 'a', 'accept', 'address', 'advic', 'an', 'ani', 'anyon', 'are', 'assist', 'be', 'by', 'bye', 'call', 'can', 'card', 'cash', 'chat', 'contact', 'credit', 'custom', 'day', 'deliveri', 'do', 'doe', 'email', 'for', 'funni', 'get', 'give', 'go', 'good', 'goodby', 'guid', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'human', 'i', 'in', 'is', 'it', 'item', 'joke', 'kind', 'know', 'later', 'laugh', 'like', 'long', 'lot', 'make', 'mastercard', 'may', 'me', 'my', 'name', 'need', 'number', 'of', 'onli', 'order', 'outsid', 'pay', 'paypal', 'phone', 'rain', 'see', 'sell', 'servic', 'ship', 'should', 'someth', 'stori', 'stuck', 'suggest', 'sunni', 'support', 'take', 'talk', 'tell', 'temperatur', 'thank', 'that', 'the', 'there', 'to', 'today', 'touch', 'track', 'umbrella', 'weather', 'what', 'when', 'which', 'who', 'will', 'with', 'you', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates empty lists to store the training data that will be used to train a natural language processing model. It then loops through each (pattern_sentence, tag) pair in the list of pairs, which contains a tokenized sentence and its corresponding tag. For each pair, the code converts the pattern sentence to a bag of words array using the bag_of_words function, and adds the resulting array to the list of X training data. It also converts the tag to a class label using the index method of the tags list, and adds the resulting label to the list of y training data. After processing all the pairs, the code converts the lists of X and y training data to numpy arrays using the np.array function. The resulting X_train and y_train arrays can be used to train a natural language processing model using a machine learning library such as PyTorch or TensorFlow."
      ],
      "metadata": {
        "id": "ZTP9vFiPgVa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty lists to store the training data\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "# Loop through each (pattern_sentence, tag) pair in the list of pairs\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    # Convert the pattern sentence to a bag of words array\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    # Add the bag of words array to the list of X training data\n",
        "    X_train.append(bag)\n",
        "    # Convert the tag to a class label\n",
        "    label = tags.index(tag)\n",
        "    # Add the class label to the list of y training data\n",
        "    y_train.append(label)\n",
        "\n",
        "# Convert the lists of X and y training data to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n"
      ],
      "metadata": {
        "id": "ToC7JX55kuv8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a PyTorch neural network module called NeuralNet, which inherits from the nn.Module class. The __init__ method of the NeuralNet class defines three linear layers with ReLU activation functions, and initializes them with the specified input size, hidden size, and number of classes. The forward method of the NeuralNet class defines the forward pass through the network, which takes an input tensor x, applies the linear layers and ReLU activations in sequence, and returns the output tensor. The output tensor does not have an activation or softmax function applied, as this will be handled by the loss function during training. This neural network architecture is a simple feedforward neural network with three hidden layers and ReLU activation functions, and can be used to classify text data into different categories based on the bag-of-words representation of the input."
      ],
      "metadata": {
        "id": "fOcicrUigny-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # Define three linear layers with ReLU activation functions\n",
        "        self.l1 = nn.Linear(input_size, hidden_size) \n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()  # Initialize the ReLU activation function\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        out = self.l1(x)  # Apply the first linear layer to the input tensor\n",
        "        out = self.relu(out)  # Apply the ReLU activation function to the output tensor\n",
        "        out = self.l2(out)  # Apply the second linear layer to the output tensor\n",
        "        out = self.relu(out)  # Apply the ReLU activation function to the output tensor\n",
        "        out = self.l3(out)  # Apply the third linear layer to the output tensor\n",
        "        # No activation or softmax at the end, as this will be handled by the loss function during training\n",
        "        return out"
      ],
      "metadata": {
        "id": "PeK_MxgwkzfC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a PyTorch Dataset class called ChatDataset, which is used to represent a dataset of input-output pairs for training a machine learning model. The __init__ method of the ChatDataset class initializes the dataset with the X_train and y_train arrays that were created earlier in the preprocessing step. The __getitem__ method of the ChatDataset class supports indexing such that dataset[i] can be used to get the i-th sample in the dataset, and returns the x and y data corresponding to the given index. The __len__ method of the ChatDataset class returns the number of samples in the dataset when len(dataset) is called. This ChatDataset class can be used to create a PyTorch DataLoader object, which can be used to batch and shuffle the training data during training of the neural network model."
      ],
      "metadata": {
        "id": "Ph66FvBbg-zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Initialize the dataset with the X and y training data\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    # Support indexing such that dataset[i] can be used to get the i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        # Return the x and y data corresponding to the given index\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # Return the number of samples in the dataset when len(dataset) is called\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "D7s-Qvg8k073"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some hyper-parameters for the model\n",
        "num_epochs = 1000  # The number of training epochs\n",
        "batch_size = 8  # The batch size used for training\n",
        "learning_rate = 0.001  # The learning rate used for training\n",
        "input_size = len(X_train[0])  # The number of features in the input data\n",
        "hidden_size = 8  # The number of neurons in the hidden layer\n",
        "output_size = len(tags)  # The number of classes in the output layer\n",
        "\n",
        "# Print some information about the dataset and model\n",
        "print(input_size, output_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awu9WkBAk1Je",
        "outputId": "e560dfe3-1a16-49e9-80ad-5ea6b562f21f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a ChatDataset object to represent the training data, and a DataLoader object to batch and shuffle the training data during training. The DataLoader object is initialized with the ChatDataset object, and the batch_size, shuffle, and num_workers arguments control the size of the batches, whether to shuffle the data during training, and the number of subprocesses used for data loading. The device variable is set to either 'cuda' or 'cpu' based on whether a CUDA-capable GPU is available for training. The model variable is created as a NeuralNet object with the specified input size, hidden size, and output size, and is moved to the appropriate device using the to method. The criterion variable is set to the cross-entropy loss function, which is commonly used for classification tasks, and the optimizer variable is set to the Adam optimization algorithm with the specified learning rate. These objects will be used to train the model in the next step."
      ],
      "metadata": {
        "id": "vUwEYDy_hYWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatDataset object to represent the training data\n",
        "dataset = ChatDataset()\n",
        "\n",
        "# Create a DataLoader object to batch and shuffle the training data during training\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "# Determine whether to use the GPU or CPU for training based on availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create a NeuralNet object to represent the neural network model and move it to the device\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# Define the loss function and optimization algorithm for training the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "-sciBJpak1WH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the neural network model on the training data using the num_epochs, train_loader, model, criterion, and optimizer objects defined earlier. The training loop iterates over the specified number of epochs, and for each epoch, iterates over the batches of input-output pairs in the train_loader. The input and label data are moved to the appropriate device, and a forward pass is performed through the network to compute the output tensor and loss. A backward pass is then performed through the network to compute the gradients of the loss with respect to the network weights, and the optimizer is used to update the weights based on the computed gradients. The loss is printed every 100 epochs to monitor the progress of the training. After training, the final loss is printed, and the trained model, input size, hidden size, output size, all words, and tags are saved to a dictionary called data. This dictionary can be used to save and load the trained model for later use in making predictions on new input data."
      ],
      "metadata": {
        "id": "9wW54EsIhgZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the training data\n",
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        # Move the input and label data to the appropriate device\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "        \n",
        "        # Perform a forward pass through the network and compute the loss\n",
        "        outputs = model(words)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Perform a backward pass through the network and update the weights\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Print the final loss after training\n",
        "print(f'final loss: {loss.item():.4f}')\n",
        "\n",
        "# Save the trained model, input size, hidden size, output size, all words, and tags to a dictionary\n",
        "data = {\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"input_size\": input_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"output_size\": output_size,\n",
        "    \"all_words\": all_words,\n",
        "    \"tags\": tags\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzRzFSc5k9NS",
        "outputId": "b31b6389-0ca0-44d0-ad6e-57eff948a0f3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.7341\n",
            "Epoch [200/1000], Loss: 0.0476\n",
            "Epoch [300/1000], Loss: 0.0078\n",
            "Epoch [400/1000], Loss: 0.0068\n",
            "Epoch [500/1000], Loss: 0.0016\n",
            "Epoch [600/1000], Loss: 0.0010\n",
            "Epoch [700/1000], Loss: 0.0012\n",
            "Epoch [800/1000], Loss: 0.0004\n",
            "Epoch [900/1000], Loss: 0.0003\n",
            "Epoch [1000/1000], Loss: 0.0002\n",
            "final loss: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model, input size, hidden size, output size, all words, and tags to a file\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "# Print a message indicating that training is complete and the file has been saved\n",
        "print(f'training complete. file saved to {FILE}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jRbmB69lAD3",
        "outputId": "61a85d33-1cbd-4cb9-e197-be246643d6d6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine whether to use the GPU or CPU for inference based on availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the intents file and trained model from disk\n",
        "with open('intents.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "# Extract the input size, hidden size, output size, all words, tags, and model state from the loaded data\n",
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "all_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "# Create a new neural network model with the same architecture as the trained model, move it to the appropriate device, and load the trained weights\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "\n",
        "# Set the model to evaluation mode to disable dropout and enable batch normalization\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxAAUry2lD24",
        "outputId": "7f408df8-6a67-44ae-8406-5756caba399f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (l1): Linear(in_features=104, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=11, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the name of the chatbot\n",
        "bot_name = \"sam\"\n",
        "\n",
        "# Print a message indicating that the chat session has started\n",
        "print(\"Let's chat! (type 'quit' to exit)\")\n",
        "\n",
        "# Start an infinite loop to read input from the user and generate responses\n",
        "while True:\n",
        "    # Read a sentence from the user\n",
        "    sentence = input(\"You: \")\n",
        "    \n",
        "    # If the user types 'quit', exit the loop\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "    \n",
        "    # Tokenize the input sentence and convert it to a bag of words vector\n",
        "    sentence = tokenize(sentence)\n",
        "    X = bag_of_words(sentence, all_words)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "    \n",
        "    # Pass the bag of words vector through the model to get a prediction\n",
        "    output = model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "    \n",
        "    # Get the tag associated with the predicted class label and the probability of the prediction\n",
        "    tag = tags[predicted.item()]\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    \n",
        "    # If the predicted probability is above a threshold, randomly select a response from the corresponding intent in the intents file\n",
        "    # Otherwise, print a message indicating that the chatbot does not understand\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents['intents']:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "    else:\n",
        "        print(f\"{bot_name}: I do not understand...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtBRACxKlIEN",
        "outputId": "300d7d21-4b5c-4765-b558-e5a5be419733"
      },
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'quit' to exit)\n",
            "You: hello\n",
            "sam: Hey :-)\n",
            "You: whats the weather today ?\n",
            "sam: I'm not able to provide weather information. How can I assist you in another way?\n",
            "You: tell me a joke ?\n",
            "sam: Why couldn't the bicycle stand up by itself? Because it was two-tired!\n",
            "You: how long is my delivery ?\n",
            "sam: Hi there, how can I help?\n",
            "You: track my order\n",
            "sam: I do not understand...\n",
            "You: what items do you have ?\n",
            "sam: We have coffee and tea\n",
            "You: good\n",
            "sam: Hello, thanks for visiting\n",
            "You: bye\n",
            "sam: Bye! Come back again soon.\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}